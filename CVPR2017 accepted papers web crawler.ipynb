{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CVPR2017 papers web crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Import and initialize main page\n",
    "import urllib2\n",
    "req = urllib2.Request('http://openaccess.thecvf.com/CVPR2017.py')\n",
    "response = urllib2.urlopen(req)\n",
    "page = response.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of all papers to process:  783\n"
     ]
    }
   ],
   "source": [
    "### Crawl from main page\n",
    "import re\n",
    "myItems = re.findall('<dt class=\"ptitle\">(.*?)</div>\\n</div>\\n</dd>',page,re.S)\n",
    "\n",
    "for item in myItems:\n",
    "    mainPages = re.findall('<br><a href=\"(.*?)\">',page,re.S)\n",
    "    pdfPages = re.findall('</dd>\\n<dd>\\n.<a href=\"content_cvpr_2017/papers/(.*?)\">pdf</a>',page,re.S)\n",
    "    firstAttempAuthors = re.findall('author = {(.*?)},<br>',page,re.S)\n",
    "    titles = re.findall('<br>\\ntitle = {(.*?)},<br>',page,re.S)\n",
    "        \n",
    "#print 'size of authors:', len(authors), '\\n'\n",
    "#print 'size of titles:' , len(titles), '\\n'\n",
    "#print 'size of mainPages:' , len(mainPages), '\\n'\n",
    "#print 'size of pdfPages:' , len(pdfPages), '\\n'\n",
    "    \n",
    "#    print authors[781]\n",
    "#    print titles[781]\n",
    "#    print mainPages\n",
    "print 'number of all papers to process: ', len(pdfPages)\n",
    "    #print pdfPages[0]\n",
    "    #return pdfPages[0]\n",
    "\n",
    "\n",
    "def getContext(url):  \n",
    "    text =urllib2.urlopen(url).read()  \n",
    "    return text  \n",
    "  \n",
    "def StoreContext(url):  \n",
    "    content  = getContext(url)  \n",
    "    filename = url[52:]  \n",
    "    open(filename, 'w').write(content) \n",
    "\n",
    "#StoreContext(url0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get author names on each paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 methods on getting the author names:\n",
    "    1. get from the main page, under each title name;\n",
    "    2. get from the paper's own page,under title name.\n",
    "    \n",
    "The problem with the second method is that some pages are no longer valid now. So I added a judgement part to test if the page is valid. Same judgement is also used when accessing the pdf pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error at paper number # 257 :\n",
      "can not reach a server at http://openaccess.thecvf.com/content_cvpr_2017/html/Ilg_FlowNet_2.0_Evolution_CVPR_2017_paper.html\n",
      "Error at paper number # 508 :\n",
      "can not reach a server at http://openaccess.thecvf.com/content_cvpr_2017/html/Yuan_BigHand2.2M_Benchmark_Hand_CVPR_2017_paper.html\n"
     ]
    }
   ],
   "source": [
    "# check for invalid pages\n",
    "from urllib2 import URLError\n",
    "result_url=[]  \n",
    "allAuthors = []\n",
    "need_second_attemp = []\n",
    "\n",
    "for i in range(len(mainPages)):\n",
    "    url = 'http://openaccess.thecvf.com/' + mainPages[i]\n",
    "    try:\n",
    "        response=urllib2.urlopen(url)\n",
    "    except URLError, e:\n",
    "        if hasattr(e,'reason'): #stands for URLError\n",
    "            print 'Error at paper number #', i, ':'\n",
    "            print \"can not reach a server at \" + url\n",
    "            result_url.append(url)\n",
    "            need_second_attemp.append(i)\n",
    "        else: #stands for HTTPError or other\n",
    "            print 'Error at paper number #', i, ':'\n",
    "            print \"find http error at \" + url\n",
    "            result_url.append(url)\n",
    "            need_second_attemp.append(i)\n",
    "    else:\n",
    "        #print \"url is reachable!\"\n",
    "        reqMain = urllib2.Request(url)\n",
    "        responseMain = urllib2.urlopen(reqMain)\n",
    "        mainPage = responseMain.read()\n",
    "        currAuthors = re.findall('<div id=\"authors\">\\n<br><b><i>(.*?)</i></b>',mainPage,re.S)[0]\n",
    "        allAuthors.append(currAuthors)\n",
    "        response.close()\n",
    "    finally:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Second attempt on the invalid pages\n",
    "if len(need_second_attemp) != 0:\n",
    "    newAllAuthors = []\n",
    "    for item in allAuthors:\n",
    "        item = str(item).split(', ')\n",
    "        newAllAuthors.append(item)\n",
    "\n",
    "    while need_second_attemp:\n",
    "        index = need_second_attemp.pop(0)\n",
    "        curr = str(firstAttempAuthors[index])\n",
    "        curr = curr.split(' and ')\n",
    "        newCurr = []\n",
    "        for item in curr:\n",
    "            item = str(item).split(', ')\n",
    "            newItem = str(item[1] + ' ' + item[0])\n",
    "            newCurr.append(newItem)\n",
    "        newAllAuthors.insert(index, newCurr)\n",
    "#for item in newAllAuthors:\n",
    "    #print item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Damien Teney', 'Lingqiao Liu', 'Anton van den Hengel']\n",
      "['Hao Zhao', 'Ming Lu', 'Anbang Yao', 'Yiwen Guo', 'Yurong Chen', 'Li Zhang']\n",
      "['Felix Juefei-Xu', 'Vishnu Naresh Boddeti', 'Marios Savvides']\n",
      "['Yagiz Aksoy', 'Tunc Ozan Aydin', 'Marc Pollefeys']\n",
      "['George Trigeorgis', 'Patrick Snape', 'Iasonas Kokkinos', 'Stefanos Zafeiriou']\n",
      "['James Booth', 'Epameinondas Antonakos', 'Stylianos Ploumpis', 'George Trigeorgis', 'Yannis Panagakis', 'Stefanos Zafeiriou']\n",
      "['Vamsi Kiran Adhikarla', 'Marek Vinkler', 'Denis Sumin', 'Rafal K. Mantiuk', 'Karol Myszkowski', 'Hans-Peter Seidel', 'Piotr Didyk']\n",
      "['Manikanta Kotaru', 'Sachin Katti']\n",
      "['Kenichiro Tanaka', 'Yasuhiro Mukaigawa', 'Takuya Funatomi', 'Hiroyuki Kubo', 'Yasuyuki Matsushita', 'Yasushi Yagi']\n",
      "['Philip Haeusser', 'Alexander Mordvintsev', 'Daniel Cremers']\n"
     ]
    }
   ],
   "source": [
    "### some examples:\n",
    "for i in range(10):\n",
    "    print newAllAuthors[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get instituitions information from papers' pdf links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    * used *pdfminer* to convert a pdf url into text;\n",
    "    * then used *urllib2* to get all the content before \"Abstract\";\n",
    "    * deleted title, author names and emails text;\n",
    "    * deleted characters that are not alphabets. (to get rid of quotation marks and such)\n",
    "\n",
    "But the current model is still not working so well with:\n",
    "\n",
    "    1. Unsual strcture:\n",
    "    \n",
    "    some papers may have figures and figure labels before the abstract, which will be interpreted as \"institution information\" in this model. Like this paper here: http://openaccess.thecvf.com/content_cvpr_2017/papers/Simon_Hand_Keypoint_Detection_CVPR_2017_paper.pdf. Which will be detected as an institution.\n",
    "        \n",
    "    2. Unexpected characters:\n",
    "    \n",
    "    some author names are not unified between paper's pdf page and paper's own page (often happens when an author is using charactors that are not included in English). For example, for this page: Towards a Quality Metric for Dense Light Fields. On its own page (http://openaccess.thecvf.com/content_cvpr_2017/html/Adhikarla_Towards_a_Quality_CVPR_2017_paper.html)(and also on the main page) the 4th author's name is listed *\"Rafal K\"*. However, in the pdf version (http://openaccess.thecvf.com/content_cvpr_2017/papers/Adhikarla_Towards_a_Quality_CVPR_2017_paper.pdf), this author's name is presented as *\"Rafał K. Mantiuk\"*. And the character *\"ł\"* cannot be detected as the same as the English character *\"l\"*. In this case, the string *\"Rafał K. Mantiuk\"* cannot match *\"Rafal K\"* (which is already acquired as one of the author names under this paper in previous step). As a result, *\"Rafał K. Mantiuk\"* won't be detected as an author name, won't be removed, and will be treated as an institution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for invalid pages (None this time)\n",
    "from urllib2 import URLError\n",
    "valid_cnt = 0\n",
    "for i in range(len(pdfPages)):\n",
    "    url = 'http://openaccess.thecvf.com/content_cvpr_2017/papers/' + pdfPages[i]\n",
    "    try:\n",
    "        response=urllib2.urlopen(url)\n",
    "    except URLError, e:\n",
    "        #print url\n",
    "        if hasattr(e,'reason'): #stands for URLError\n",
    "            print 'paper number #', i\n",
    "            print \"can not reach a server at \" + url\n",
    "        else: #stands for HTTPError or other\n",
    "            print 'paper number #', i\n",
    "            print \"find http error at \" + url\n",
    "    else:\n",
    "        valid_cnt += 1\n",
    "        \n",
    "    finally:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from cStringIO import StringIO\n",
    "\n",
    "def Pdf_Write2txt(html):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "\n",
    "    retstr = StringIO()\n",
    "    laparams=LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec='utf-8', laparams=laparams)\n",
    "\n",
    "    fp = StringIO(html)\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    pagenos=set()\n",
    "    \n",
    "    for page in PDFPage.get_pages(fp, pagenos, maxpages=0, password=\"\",caching=True, check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "        break\n",
    "    fp.close()\n",
    "    device.close()\n",
    "    currPage = retstr.getvalue()\n",
    "    retstr.close()\n",
    "    return currPage\n",
    "    \n",
    "def get_pdf_url(url):\n",
    "    url = 'http://openaccess.thecvf.com/content_cvpr_2017/papers/' + url\n",
    "    html = urllib2.urlopen(urllib2.Request(url)).read()\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get institution information\n",
    "allInst = []\n",
    "for num_paper in range(len(pdfPages)):\n",
    "\n",
    "    currPage = Pdf_Write2txt(get_pdf_url(pdfPages[num_paper]))\n",
    "    hasAbstract = bool(re.search('abstract', currPage, re.IGNORECASE)) \n",
    "\n",
    "    if not hasAbstract:\n",
    "        allInst.append(str('cannot detect Abstract at: ' + str(num_paper)))\n",
    "    else:\n",
    "        abSplitPage = re.split('abstract', currPage, flags=re.I)[0]\n",
    "        \n",
    "        ########\n",
    "        ### transfer all to lower case, and remove all symbols\n",
    "        lowerPage = abSplitPage.lower()\n",
    "        lowerPage = re.sub('ﬁ', 'fi', lowerPage)\n",
    "        noSymbolPage = re.sub(r'[^a-zA-Z0-9\\s@,]',r'',lowerPage)\n",
    "\n",
    "        ########\n",
    "        ### remove author names\n",
    "        for item in newAllAuthors[num_paper]:\n",
    "            name = item.lower()\n",
    "            name = re.sub(r'[^a-zA-Z0-9\\s@,]',r'',name)\n",
    "            #print name\n",
    "            noSymbolPage = re.sub(str(name), '', noSymbolPage)\n",
    "\n",
    "        ########\n",
    "        ### remove title\n",
    "        title = titles[num_paper].lower()\n",
    "        title = re.sub(r'[^a-zA-Z0-9\\s@,]',r'',title)\n",
    "        title = re.sub(' ', '\\s+', title)\n",
    "        newLowerPage = re.sub(str(title), '', noSymbolPage, re.S)\n",
    "        \n",
    "        #########\n",
    "        ### remove emails and citation marks\n",
    "        newLowerPage2 = re.sub('\\s.+@.+','',newLowerPage,re.S)\n",
    "        newLowerPage3 = re.sub(r'[^a-zA-Z\\s\\-]',r'',newLowerPage2)\n",
    "        emptyLineCount = 0\n",
    "        \n",
    "        lineSplitPage = newLowerPage3.split('\\n')\n",
    "        institution = []\n",
    "        for line in lineSplitPage:\n",
    "            line = re.sub('\\s+and\\s+','',line)\n",
    "            line = line.strip()\n",
    "            if len(line) == 0 or line == '' or line == ' ':\n",
    "                emptyLineCount += 1\n",
    "            else:\n",
    "                line = re.sub('\\r*\\n*\\f*\\t*\\v*', '', line)\n",
    "                institution.append(line)\n",
    "        #print institution\n",
    "        allInst.append(institution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['australian centre for visual technologies', 'the university of adelaide']\n",
      "['department of electronic engineering tsinghua university', 'cognitive computing laboratory intel labs china']\n",
      "['carnegie mellon university', 'michigan state university', 'carnegie mellon university']\n",
      "['yagz aksoy tunc ozan aydn', 'eth zurich', 'disney research zurich']\n",
      "['imperial college london', 'imperial college london', 'university college london', 'imperial college london']\n",
      "['imperial college london uk', 'amazon berlin germany', 'university of oulu finland']\n",
      "['rafa k mantiuk', 'mpi informatik', 'saarland university mmci', 'the computer laboratory university of cambridge']\n",
      "['stanford university']\n",
      "['nara institute of sciencetechnology naist', 'osaka univerisity']\n",
      "['dept of informatics tu munich', 'google inc']\n"
     ]
    }
   ],
   "source": [
    "### some examples:\n",
    "for i in range(10):\n",
    "    print allInst[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Get topics from analysing papers' abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used Topic modeling to get 3 topic keywords from abstract of each paper.\n",
    "\n",
    "    * converted abstract into a matrix representation;\n",
    "    * create an object for LDA model using gensim library;\n",
    "    * run and train the LDA model on the term matrix;\n",
    "    * get 1 topic with 3 keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "allTopics = []\n",
    "for i in range(len(mainPages)):\n",
    "    if i == 257 or i == 508:\n",
    "        allTopics.append(titles[i])\n",
    "    else:\n",
    "        url = 'http://openaccess.thecvf.com/' + mainPages[i]\n",
    "\n",
    "        #print \"url is reachable!\"\n",
    "        reqMain = urllib2.Request(url)\n",
    "        responseMain = urllib2.urlopen(reqMain)\n",
    "        mainPage = responseMain.read()\n",
    "        abstract = re.split('id=\"abstract\"', mainPage, flags=re.I)[1]\n",
    "        abstract = re.split('<font size=\"5\">', abstract, flags=re.I)[0]\n",
    "        abstract = re.split('</div>\\s*', abstract, flags=re.I)[0]\n",
    "        #print abstract\n",
    "        abstract = str(abstract).split('.')\n",
    "        doc_complete = []\n",
    "        for item in abstract:\n",
    "            newItem = re.sub(r'[^a-zA-Z0-9 @,]',r'',item)\n",
    "            newItem = newItem.strip()\n",
    "            if newItem != '':\n",
    "                doc_complete.append(newItem)\n",
    "        #print doc_complete\n",
    "\n",
    "\n",
    "        doc_clean = [clean(doc).split() for doc in doc_complete]\n",
    "        #print doc_clean\n",
    "\n",
    "        # Importing Gensim\n",
    "        import gensim\n",
    "        from gensim import corpora\n",
    "        from gensim.corpora import Dictionary\n",
    "\n",
    "        dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "        # Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)\n",
    "        # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "        doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "\n",
    "        # Creating the object for LDA model using gensim library\n",
    "        Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "        # Running and Trainign LDA model on the document term matrix.\n",
    "        ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)\n",
    "\n",
    "        topic = ldamodel.print_topics(num_topics=1, num_words=3)\n",
    "        #print topic[0][1]\n",
    "        allTopics.append(topic[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.055*\"scene\" + 0.038*\"accuracy\" + 0.038*\"question\"\n",
      "0.051*\"method\" + 0.027*\"network\" + 0.027*\"layout\"\n",
      "0.042*\"lbc\" + 0.042*\"binary\" + 0.029*\"cnns\"\n",
      "0.044*\"region\" + 0.030*\"unknown\" + 0.030*\"information\"\n",
      "0.035*\"convolutional\" + 0.035*\"fully\" + 0.035*\"method\"\n",
      "0.050*\"facial\" + 0.050*\"model\" + 0.038*\"shape\"\n",
      "0.058*\"quality\" + 0.041*\"lightfield\" + 0.023*\"existing\"\n",
      "0.040*\"vr\" + 0.040*\"g\" + 0.023*\"e\"\n",
      "0.064*\"material\" + 0.052*\"object\" + 0.052*\"camera\"\n",
      "0.044*\"data\" + 0.044*\"unlabeled\" + 0.031*\"use\"\n"
     ]
    }
   ],
   "source": [
    "### some examples:\n",
    "for i in range(10):\n",
    "    print allTopics[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Export data into txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileObject = open('All_Authors.txt', 'w')\n",
    "for item in newAllAuthors:\n",
    "    for ip in item:\n",
    "        fileObject.write(ip)\n",
    "        fileObject.write(', ')\n",
    "    fileObject.write('\\n')\n",
    "fileObject.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileObject = open('All_Topics.txt', 'w')\n",
    "for ip in allTopics:\n",
    "    fileObject.write(ip)\n",
    "    fileObject.write('\\n')\n",
    "fileObject.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileObject = open('All_Institution.txt', 'w')\n",
    "for item in allInst:\n",
    "    for ip in item:\n",
    "        fileObject.write(ip)\n",
    "        fileObject.write(', ')\n",
    "    fileObject.write('\\n')\n",
    "fileObject.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
